<!DOCTYPE html>

<html lang="en" data-content_root="../../../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />



  <meta name="viewport" content="width=device-width,initial-scale=1">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="lang:clipboard.copy" content="Copy to clipboard">
  <meta name="lang:clipboard.copied" content="Copied to clipboard">
  <meta name="lang:search.language" content="en">
  <meta name="lang:search.pipeline.stopwords" content="True">
  <meta name="lang:search.pipeline.trimmer" content="True">
  <meta name="lang:search.result.none" content="No matching documents">
  <meta name="lang:search.result.one" content="1 matching document">
  <meta name="lang:search.result.other" content="# matching documents">
  <meta name="lang:search.tokenizer" content="[\s\-]+">

  
    <link href="https://fonts.gstatic.com/" rel="preconnect" crossorigin>
    <link href="https://fonts.googleapis.com/css?family=Roboto+Mono:400,500,700|Roboto:300,400,400i,700&display=fallback" rel="stylesheet">

    <style>
      body,
      input {
        font-family: "Roboto", "Helvetica Neue", Helvetica, Arial, sans-serif
      }

      code,
      kbd,
      pre {
        font-family: "Roboto Mono", "Courier New", Courier, monospace
      }
    </style>
  

  <link rel="stylesheet" href="../../../../_static/stylesheets/application.css"/>
  <link rel="stylesheet" href="../../../../_static/stylesheets/application-palette.css"/>
  <link rel="stylesheet" href="../../../../_static/stylesheets/application-fixes.css"/>
  
  <link rel="stylesheet" href="../../../../_static/fonts/material-icons.css"/>
  
  <meta name="theme-color" content="#3f51b5">
  <script src="../../../../_static/javascripts/modernizr.js"></script>
  
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-XXXXX"></script>
<script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
        dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'UA-XXXXX');
</script>
  
  
    <title>graphnet.models.components.layers &#8212; graphnet  documentation</title>

<style>
  dt:target {
    margin-top: 0;
    padding-top: 0;
  }


  /*
    .sig-prename {
     display: none;
  }
  */

  .py.class .sig-name,
  .py.function .sig-name,
  .py.method .sig-name,
  .py.exception .sig-name {
    color: #37474f;
    font-feature-settings: "kern";
    font-family: "Roboto Mono", "Courier New", Courier, monospace;
    font-weight: 700;
  }

  .py.class .sig-object,
  .py.function .sig-object,
  .py.method .sig-object,
  .py.exception .sig-object {
    padding: 1ex;
  }

  .py.class .sig-object,
  .py.function .sig-object,
  .py.exception .sig-object {
    border-top: 1px solid gray;
  }
  .py.method .sig-object {
    border-top: 1px solid lightgray;
  }

  .py.class .sig-object,
  .py.exception .sig-object {
    background: rgba(0,0,0,0.06);
  }
  .py.function .sig-object,
  .py.method .sig-object {
    background: rgba(0,0,0,0.03);
  }

  #eu-emblem {
    margin: 0;
  }

  #eu-emblem figcaption {
    display: none;
  }

</style>
    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=75810a84" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/material.css?v=79c92029" />
    <script src="../../../../_static/documentation_options.js?v=5929fcd5"></script>
    <script src="../../../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="icon" href="../../../../_static/favicon.svg"/>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
  
   

  </head>
  <body dir=ltr
        data-md-color-primary=indigo data-md-color-accent=blue>
  
  <svg class="md-svg">
    <defs data-children-count="0">
      
      <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="__github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg>
      
    </defs>
  </svg>
  
  <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer">
  <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search">
  <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
  <a href="#_modules/graphnet/models/components/layers" tabindex="1" class="md-skip"> Skip to content </a>
  <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex navheader">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="../../../../index.html" title="graphnet  documentation"
           class="md-header-nav__button md-logo">
          
            &nbsp;
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          <span class="md-header-nav__topic">GraphNeT</span>
          <span class="md-header-nav__topic"> graphnet.models.components.layers </span>
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
        
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" action="../../../../search.html" method="get" name="search">
      <input type="text" class="md-search__input" name="q" placeholder=""Search""
             autocapitalize="off" autocomplete="off" spellcheck="false"
             data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>

      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            <a href="https://github.com/graphnet-team/graphnet/" title="Go to repository" class="md-source" data-md-source="github">

    <div class="md-source__icon">
      <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 24 24" width="28" height="28">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    GraphNeT
  </div>
</a>
          </div>
        </div>
      
      
  
  <script src="../../../../_static/javascripts/version_dropdown.js"></script>
  <script>
    var json_loc = "../../../../"versions.json"",
        target_loc = "../../../../../",
        text = "Versions";
    $( document ).ready( add_version_dropdown(json_loc, target_loc, text));
  </script>
  

    </div>
  </nav>
</header>

  
  <div class="md-container">
    
    
    
  <nav class="md-tabs" data-md-component="tabs">
    <div class="md-tabs__inner md-grid">
      <ul class="md-tabs__list">
            
            <li class="md-tabs__item"><a href="../../../../index.html" class="md-tabs__link">Documentation</a></li>
          <li class="md-tabs__item"><a href="../../../index.html" class="md-tabs__link">Module code</a></li>
      </ul>
    </div>
  </nav>
    <main class="md-main">
      <div class="md-main__inner md-grid" data-md-component="container">
        
          <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
            <div class="md-sidebar__scrollwrap">
              <div class="md-sidebar__inner">
                <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="../../../../index.html" title="graphnet documentation" class="md-nav__button md-logo">
      
        <img src="../../../../_static/" alt=" logo" width="48" height="48">
      
    </a>
    <a href="../../../../index.html"
       title="graphnet documentation">GraphNeT</a>
  </label>
    <div class="md-nav__source">
      <a href="https://github.com/graphnet-team/graphnet/" title="Go to repository" class="md-source" data-md-source="github">

    <div class="md-source__icon">
      <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 24 24" width="28" height="28">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    GraphNeT
  </div>
</a>
    </div>
  
  

  
  <ul class="md-nav__list">
    <li class="md-nav__item">
    
    
      <a href="../../../../installation/install.html" class="md-nav__link">Installation</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../../../models/models.html" class="md-nav__link">Models In GraphNeT</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../../../datasets/datasets.html" class="md-nav__link">Datasets In GraphNeT</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../../../data_conversion/data_conversion.html" class="md-nav__link">Data Conversion in GraphNeT</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../../../integration/integration.html" class="md-nav__link">Integrating New Experiments into GraphNeT</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../../../contribute/contribute.html" class="md-nav__link">Contributing To GraphNeT</a>
      
    
    </li>
    <li class="md-nav__item">
    
    
      <a href="../../../../api/graphnet.html" class="md-nav__link">API</a>
      
    
    </li>
  </ul>
  

</nav>
              </div>
            </div>
          </div>
          <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
            <div class="md-sidebar__scrollwrap">
              <div class="md-sidebar__inner">
                
<nav class="md-nav md-nav--secondary">
  <ul class="md-nav__list" data-md-scrollfix="">
  </ul>
</nav>
              </div>
            </div>
          </div>
        
        <div class="md-content">
          <article class="md-content__inner md-typeset" role="main">
            
  <h1 id="modules-graphnet-models-components-layers--page-root">Source code for graphnet.models.components.layers</h1><div class="highlight"><pre>
<span></span><span class="sd">"""Class(es) implementing layers to be used in `graphnet` models."""</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">,</span> <span class="n">Union</span><span class="p">,</span> <span class="n">List</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.functional</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch_geometric.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">EdgeConv</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch_geometric.nn.pool</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">knn_graph</span><span class="p">,</span>
    <span class="n">global_mean_pool</span><span class="p">,</span>
    <span class="n">global_add_pool</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch_geometric.typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Adj</span><span class="p">,</span> <span class="n">PairTensor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch_geometric.nn.conv</span><span class="w"> </span><span class="kn">import</span> <span class="n">MessagePassing</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch_geometric.nn.inits</span><span class="w"> </span><span class="kn">import</span> <span class="n">reset</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch_geometric.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">Data</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="kn">import</span> <span class="n">linear</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.modules</span><span class="w"> </span><span class="kn">import</span> <span class="n">TransformerEncoder</span><span class="p">,</span> <span class="n">TransformerEncoderLayer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch_geometric.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">to_dense_batch</span><span class="p">,</span> <span class="n">softmax</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch_scatter</span><span class="w"> </span><span class="kn">import</span> <span class="n">scatter</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">pytorch_lightning</span><span class="w"> </span><span class="kn">import</span> <span class="n">LightningModule</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch_geometric.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">degree</span>


<div class="viewcode-block" id="DynEdgeConv">
<a class="viewcode-back" href="../../../../api/graphnet.models.components.layers.html#graphnet.models.components.layers.DynEdgeConv">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">DynEdgeConv</span><span class="p">(</span><span class="n">EdgeConv</span><span class="p">,</span> <span class="n">LightningModule</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Dynamical edge convolution layer."""</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">nn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">aggr</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">"max"</span><span class="p">,</span>
        <span class="n">nb_neighbors</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
        <span class="n">features_subset</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="nb">slice</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">"""Construct `DynEdgeConv`.</span>

<span class="sd">        Args:</span>
<span class="sd">            nn: The MLP/torch.Module to be used within the `EdgeConv`.</span>
<span class="sd">            aggr: Aggregation method to be used with `EdgeConv`.</span>
<span class="sd">            nb_neighbors: Number of neighbours to be clustered after the</span>
<span class="sd">                `EdgeConv` operation.</span>
<span class="sd">            features_subset: Subset of features in `Data.x` that should be used</span>
<span class="sd">                when dynamically performing the new graph clustering after the</span>
<span class="sd">                `EdgeConv` operation. Defaults to all features.</span>
<span class="sd">            **kwargs: Additional features to be passed to `EdgeConv`.</span>
<span class="sd">        """</span>
        <span class="c1"># Check(s)</span>
        <span class="k">if</span> <span class="n">features_subset</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">features_subset</span> <span class="o">=</span> <span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>  <span class="c1"># Use all features</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">features_subset</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">slice</span><span class="p">))</span>

        <span class="c1"># Base class constructor</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">nn</span><span class="o">=</span><span class="n">nn</span><span class="p">,</span> <span class="n">aggr</span><span class="o">=</span><span class="n">aggr</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># Additional member variables</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nb_neighbors</span> <span class="o">=</span> <span class="n">nb_neighbors</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">features_subset</span> <span class="o">=</span> <span class="n">features_subset</span>

<div class="viewcode-block" id="DynEdgeConv.forward">
<a class="viewcode-back" href="../../../../api/graphnet.models.components.layers.html#graphnet.models.components.layers.DynEdgeConv.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">:</span> <span class="n">Adj</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Forward pass."""</span>
        <span class="c1"># Standard EdgeConv forward pass</span>
        <span class="n">x</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">)</span>

        <span class="c1"># Recompute adjacency</span>
        <span class="n">edge_index</span> <span class="o">=</span> <span class="n">knn_graph</span><span class="p">(</span>
            <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">features_subset</span><span class="p">],</span>
            <span class="n">k</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">nb_neighbors</span><span class="p">,</span>
            <span class="n">batch</span><span class="o">=</span><span class="n">batch</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span></div>
</div>



<div class="viewcode-block" id="EdgeConvTito">
<a class="viewcode-back" href="../../../../api/graphnet.models.components.layers.html#graphnet.models.components.layers.EdgeConvTito">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">EdgeConvTito</span><span class="p">(</span><span class="n">MessagePassing</span><span class="p">,</span> <span class="n">LightningModule</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Implementation of EdgeConvTito layer used in TITO solution for.</span>

<span class="sd">    'IceCube - Neutrinos in Deep' kaggle competition.</span>
<span class="sd">    """</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">nn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">aggr</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">"max"</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">"""Construct `EdgeConvTito`.</span>

<span class="sd">        Args:</span>
<span class="sd">            nn: The MLP/torch.Module to be used within the `EdgeConvTito`.</span>
<span class="sd">            aggr: Aggregation method to be used with `EdgeConvTito`.</span>
<span class="sd">            **kwargs: Additional features to be passed to `EdgeConvTito`.</span>
<span class="sd">        """</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">aggr</span><span class="o">=</span><span class="n">aggr</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nn</span> <span class="o">=</span> <span class="n">nn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>

<div class="viewcode-block" id="EdgeConvTito.reset_parameters">
<a class="viewcode-back" href="../../../../api/graphnet.models.components.layers.html#graphnet.models.components.layers.EdgeConvTito.reset_parameters">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Reset all learnable parameters of the module."""</span>
        <span class="n">reset</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nn</span><span class="p">)</span></div>


<div class="viewcode-block" id="EdgeConvTito.forward">
<a class="viewcode-back" href="../../../../api/graphnet.models.components.layers.html#graphnet.models.components.layers.EdgeConvTito.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">PairTensor</span><span class="p">],</span> <span class="n">edge_index</span><span class="p">:</span> <span class="n">Adj</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Forward pass."""</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="c1"># propagate_type: (x: PairTensor)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">propagate</span><span class="p">(</span><span class="n">edge_index</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></div>


<div class="viewcode-block" id="EdgeConvTito.message">
<a class="viewcode-back" href="../../../../api/graphnet.models.components.layers.html#graphnet.models.components.layers.EdgeConvTito.message">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">message</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_i</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x_j</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Edgeconvtito message passing."""</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">nn</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x_i</span><span class="p">,</span> <span class="n">x_j</span> <span class="o">-</span> <span class="n">x_i</span><span class="p">,</span> <span class="n">x_j</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>  <span class="c1"># EdgeConvTito</span></div>


    <span class="k">def</span><span class="w"> </span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Print out module name."""</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">(nn=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">nn</span><span class="si">}</span><span class="s2">)"</span></div>



<div class="viewcode-block" id="DynTrans">
<a class="viewcode-back" href="../../../../api/graphnet.models.components.layers.html#graphnet.models.components.layers.DynTrans">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">DynTrans</span><span class="p">(</span><span class="n">EdgeConvTito</span><span class="p">,</span> <span class="n">LightningModule</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Implementation of dynTrans1 layer used in TITO solution for.</span>

<span class="sd">    'IceCube - Neutrinos in Deep' kaggle competition.</span>
<span class="sd">    """</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">layer_sizes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">aggr</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">"max"</span><span class="p">,</span>
        <span class="n">features_subset</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="nb">slice</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">n_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">"""Construct `DynTrans`.</span>

<span class="sd">        Args:</span>
<span class="sd">            layer_sizes: List of layer sizes to be used in `DynTrans`.</span>
<span class="sd">            aggr: Aggregation method to be used with `DynTrans`.</span>
<span class="sd">            features_subset: Subset of features in `Data.x` that should be used</span>
<span class="sd">                when dynamically performing the new graph clustering after the</span>
<span class="sd">                `EdgeConv` operation. Defaults to all features.</span>
<span class="sd">            n_head: Number of heads to be used in the multiheadattention</span>
<span class="sd">                models.</span>
<span class="sd">            **kwargs: Additional features to be passed to `DynTrans`.</span>
<span class="sd">        """</span>
        <span class="c1"># Check(s)</span>
        <span class="k">if</span> <span class="n">features_subset</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">features_subset</span> <span class="o">=</span> <span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>  <span class="c1"># Use all features</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">features_subset</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">slice</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">layer_sizes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">layer_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">]</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">ix</span><span class="p">,</span> <span class="p">(</span><span class="n">nb_in</span><span class="p">,</span> <span class="n">nb_out</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
            <span class="nb">zip</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">layer_sizes</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
        <span class="p">):</span>
            <span class="k">if</span> <span class="n">ix</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">nb_in</span> <span class="o">*=</span> <span class="mi">3</span>  <span class="c1"># edgeConv1</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">nb_in</span><span class="p">,</span> <span class="n">nb_out</span><span class="p">))</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">())</span>
        <span class="n">d_model</span> <span class="o">=</span> <span class="n">nb_out</span>

        <span class="c1"># Base class constructor</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">nn</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">),</span> <span class="n">aggr</span><span class="o">=</span><span class="n">aggr</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># Additional member variables</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">features_subset</span> <span class="o">=</span> <span class="n">features_subset</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>  <span class="c1"># lNorm</span>

        <span class="c1"># Transformer layer(s)</span>
        <span class="n">encoder_layer</span> <span class="o">=</span> <span class="n">TransformerEncoderLayer</span><span class="p">(</span>
            <span class="n">d_model</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span>
            <span class="n">nhead</span><span class="o">=</span><span class="n">n_head</span><span class="p">,</span>
            <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">norm_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_transformer_encoder</span> <span class="o">=</span> <span class="n">TransformerEncoder</span><span class="p">(</span>
            <span class="n">encoder_layer</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span>
        <span class="p">)</span>

<div class="viewcode-block" id="DynTrans.forward">
<a class="viewcode-back" href="../../../../api/graphnet.models.components.layers.html#graphnet.models.components.layers.DynTrans.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">:</span> <span class="n">Adj</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Forward pass."""</span>
        <span class="n">x_out</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">x_out</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">x_out</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x_out</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># lNorm</span>

        <span class="c1"># Transformer layer</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">to_dense_batch</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transformer_encoder</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">src_key_padding_mask</span><span class="o">=~</span><span class="n">mask</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">x</span></div>
</div>



<div class="viewcode-block" id="DropPath">
<a class="viewcode-back" href="../../../../api/graphnet.models.components.layers.html#graphnet.models.components.layers.DropPath">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">DropPath</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Drop paths (Stochastic Depth) per sample."""</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">drop_prob</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">"""Construct `DropPath`.</span>

<span class="sd">        Args:</span>
<span class="sd">            drop_prob: Probability of dropping a path during training.</span>
<span class="sd">                If 0.0, no paths are dropped. Defaults to None.</span>
<span class="sd">        """</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DropPath</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">drop_prob</span> <span class="o">=</span> <span class="n">drop_prob</span>

<div class="viewcode-block" id="DropPath.forward">
<a class="viewcode-back" href="../../../../api/graphnet.models.components.layers.html#graphnet.models.components.layers.DropPath.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Forward pass."""</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_prob</span> <span class="o">==</span> <span class="mf">0.0</span> <span class="ow">or</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x</span>
        <span class="n">keep_prob</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_prob</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">random_tensor</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">new_empty</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">bernoulli_</span><span class="p">(</span><span class="n">keep_prob</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">keep_prob</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">:</span>
            <span class="n">random_tensor</span><span class="o">.</span><span class="n">div_</span><span class="p">(</span><span class="n">keep_prob</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">random_tensor</span></div>


<div class="viewcode-block" id="DropPath.extra_repr">
<a class="viewcode-back" href="../../../../api/graphnet.models.components.layers.html#graphnet.models.components.layers.DropPath.extra_repr">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Return extra representation of the module."""</span>
        <span class="k">return</span> <span class="s2">"p=</span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">drop_prob</span><span class="p">)</span></div>
</div>



<div class="viewcode-block" id="Mlp">
<a class="viewcode-back" href="../../../../api/graphnet.models.components.layers.html#graphnet.models.components.layers.Mlp">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">Mlp</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Multi-Layer Perceptron (MLP) module."""</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">hidden_features</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">out_features</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">activation</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">,</span>
        <span class="n">dropout_prob</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">"""Construct `Mlp`.</span>

<span class="sd">        Args:</span>
<span class="sd">            in_features: Number of input features.</span>
<span class="sd">            hidden_features: Number of hidden features. Defaults to None.</span>
<span class="sd">                If None, it is set to the value of `in_features`.</span>
<span class="sd">            out_features: Number of output features. Defaults to None.</span>
<span class="sd">                If None, it is set to the value of `in_features`.</span>
<span class="sd">            activation: Activation layer. Defaults to `nn.GELU`.</span>
<span class="sd">            dropout_prob: Dropout probability. Defaults to 0.0.</span>
<span class="sd">        """</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">in_features</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">"in_features must be greater than 0, got in_features "</span>
                <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">in_features</span><span class="si">}</span><span class="s2"> instead"</span>
            <span class="p">)</span>
        <span class="n">out_features</span> <span class="o">=</span> <span class="n">out_features</span> <span class="ow">or</span> <span class="n">in_features</span>
        <span class="n">hidden_features</span> <span class="o">=</span> <span class="n">hidden_features</span> <span class="ow">or</span> <span class="n">in_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_projection</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">hidden_features</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">activation</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_projection</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_prob</span><span class="p">)</span>

<div class="viewcode-block" id="Mlp.forward">
<a class="viewcode-back" href="../../../../api/graphnet.models.components.layers.html#graphnet.models.components.layers.Mlp.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Forward pass."""</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_projection</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_projection</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span></div>
</div>



<div class="viewcode-block" id="Block_rel">
<a class="viewcode-back" href="../../../../api/graphnet.models.components.layers.html#graphnet.models.components.layers.Block_rel">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">Block_rel</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Implementation of BEiTv2 Block."""</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">mlp_ratio</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">4.0</span><span class="p">,</span>
        <span class="n">qkv_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">qk_scale</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">attn_drop</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">drop_path</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">init_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">activation</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">,</span>
        <span class="n">norm_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">,</span>
        <span class="n">attn_head_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">"""Construct 'Block_rel'.</span>

<span class="sd">        Args:</span>
<span class="sd">            input_dim: Dimension of the input tensor.</span>
<span class="sd">            num_heads: Number of attention heads to use in the `Attention_rel`</span>
<span class="sd">            layer.</span>
<span class="sd">            mlp_ratio: Ratio of the hidden size of the feedforward network to</span>
<span class="sd">                the input size in the `Mlp` layer.</span>
<span class="sd">            qkv_bias: Whether or not to include bias terms in the query, key,</span>
<span class="sd">                and value matrices in the `Attention_rel` layer.</span>
<span class="sd">            qk_scale: Scaling factor for the dot product of the query and key</span>
<span class="sd">                matrices in the `Attention_rel` layer.</span>
<span class="sd">            dropout: Dropout probability to use in the `Mlp` layer.</span>
<span class="sd">            attn_drop: Dropout probability to use in the `Attention_rel` layer.</span>
<span class="sd">            drop_path: Probability of applying drop path regularization to the</span>
<span class="sd">                output of the layer.</span>
<span class="sd">            init_values: Initial value to use for the `gamma_1` and `gamma_2`</span>
<span class="sd">                parameters if not `None`.</span>
<span class="sd">            activation: Activation function to use in the `Mlp` layer.</span>
<span class="sd">            norm_layer: Normalization layer to use.</span>
<span class="sd">            attn_head_dim: Dimension of the attention head outputs in the</span>
<span class="sd">                `Attention_rel` layer.</span>
<span class="sd">        """</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">norm_layer</span><span class="p">(</span><span class="n">input_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">Attention_rel</span><span class="p">(</span>
            <span class="n">input_dim</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="p">,</span>
            <span class="n">attn_drop</span><span class="o">=</span><span class="n">attn_drop</span><span class="p">,</span>
            <span class="n">qkv_bias</span><span class="o">=</span><span class="n">qkv_bias</span><span class="p">,</span>
            <span class="n">qk_scale</span><span class="o">=</span><span class="n">qk_scale</span><span class="p">,</span>
            <span class="n">attn_head_dim</span><span class="o">=</span><span class="n">attn_head_dim</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">drop_path</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">DropPath</span><span class="p">(</span><span class="n">drop_path</span><span class="p">)</span> <span class="k">if</span> <span class="n">drop_path</span> <span class="o">&gt;</span> <span class="mf">0.0</span> <span class="k">else</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">norm_layer</span><span class="p">(</span><span class="n">input_dim</span><span class="p">)</span>
        <span class="n">mlp_hidden_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">input_dim</span> <span class="o">*</span> <span class="n">mlp_ratio</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">Mlp</span><span class="p">(</span>
            <span class="n">in_features</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
            <span class="n">hidden_features</span><span class="o">=</span><span class="n">mlp_hidden_dim</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
            <span class="n">dropout_prob</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">init_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gamma_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
                <span class="n">init_values</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">input_dim</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gamma_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
                <span class="n">init_values</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">input_dim</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gamma_1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma_2</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>

<div class="viewcode-block" id="Block_rel.forward">
<a class="viewcode-back" href="../../../../api/graphnet.models.components.layers.html#graphnet.models.components.layers.Block_rel.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">key_padding_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">rel_pos_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">kv</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Forward pass."""</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma_1</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">xn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">kv</span> <span class="o">=</span> <span class="n">xn</span> <span class="k">if</span> <span class="n">kv</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">kv</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_path</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="p">(</span>
                    <span class="n">xn</span><span class="p">,</span>
                    <span class="n">kv</span><span class="p">,</span>
                    <span class="n">kv</span><span class="p">,</span>
                    <span class="n">rel_pos_bias</span><span class="o">=</span><span class="n">rel_pos_bias</span><span class="p">,</span>
                    <span class="n">key_padding_mask</span><span class="o">=</span><span class="n">key_padding_mask</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_path</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mlp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">xn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">kv</span> <span class="o">=</span> <span class="n">xn</span> <span class="k">if</span> <span class="n">kv</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">kv</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_path</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">gamma_1</span>
                <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_path</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="p">(</span>
                        <span class="n">xn</span><span class="p">,</span>
                        <span class="n">kv</span><span class="p">,</span>
                        <span class="n">kv</span><span class="p">,</span>
                        <span class="n">rel_pos_bias</span><span class="o">=</span><span class="n">rel_pos_bias</span><span class="p">,</span>
                        <span class="n">key_padding_mask</span><span class="o">=</span><span class="n">key_padding_mask</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="p">)</span>
            <span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_path</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gamma_2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="k">return</span> <span class="n">x</span></div>
</div>



<div class="viewcode-block" id="Attention_rel">
<a class="viewcode-back" href="../../../../api/graphnet.models.components.layers.html#graphnet.models.components.layers.Attention_rel">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">Attention_rel</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Attention mechanism with relative position bias."""</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
        <span class="n">qkv_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">qk_scale</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">attn_drop</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">proj_drop</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">attn_head_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">"""Construct 'Attention_rel'.</span>

<span class="sd">        Args:</span>
<span class="sd">            input_dim: Dimension of the input tensor.</span>
<span class="sd">            num_heads: the number of attention heads to use (default: 8)</span>
<span class="sd">            qkv_bias: whether to add bias to the query, key, and value</span>
<span class="sd">                projections. Defaults to False.</span>
<span class="sd">            qk_scale: a scaling factor that multiplies the dot product of query</span>
<span class="sd">                and key vectors. Defaults to None. If None, computed as</span>
<span class="sd">                :math: `head_dim^(-1/2)`.</span>
<span class="sd">            attn_drop: the dropout probability for the attention weights.</span>
<span class="sd">                Defaults to 0.0.</span>
<span class="sd">            proj_drop: the dropout probability for the output of the attention</span>
<span class="sd">                module. Defaults to 0.0.</span>
<span class="sd">            attn_head_dim: the feature dimensionality of each attention head.</span>
<span class="sd">                Defaults to None. If None, computed as `dim // num_heads`.</span>
<span class="sd">        """</span>
        <span class="k">if</span> <span class="n">input_dim</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">num_heads</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">"dim and num_heads must be greater than 0,"</span>
                <span class="sa">f</span><span class="s2">" got input_dim=</span><span class="si">{</span><span class="n">input_dim</span><span class="si">}</span><span class="s2"> and num_heads=</span><span class="si">{</span><span class="n">num_heads</span><span class="si">}</span><span class="s2"> instead"</span>
            <span class="p">)</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">attn_head_dim</span> <span class="ow">or</span> <span class="n">input_dim</span> <span class="o">//</span> <span class="n">num_heads</span>
        <span class="n">all_head_dim</span> <span class="o">=</span> <span class="n">head_dim</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">qk_scale</span> <span class="ow">or</span> <span class="n">head_dim</span><span class="o">**-</span><span class="mf">0.5</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">proj_q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">all_head_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">proj_k</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">all_head_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">proj_v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">all_head_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">qkv_bias</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">q_bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">all_head_dim</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">v_bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">all_head_dim</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">q_bias</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">v_bias</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">attn_drop</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">attn_drop</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">all_head_dim</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">proj_drop</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">proj_drop</span><span class="p">)</span>

<div class="viewcode-block" id="Attention_rel.forward">
<a class="viewcode-back" href="../../../../api/graphnet.models.components.layers.html#graphnet.models.components.layers.Attention_rel.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">q</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">k</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">v</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">rel_pos_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">key_padding_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Forward pass."""</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">event_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span>

        <span class="n">q</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">q</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">proj_q</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">q_bias</span><span class="p">)</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">event_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span>
            <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span>
        <span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">proj_k</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span>
            <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span>
        <span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">v</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">proj_v</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">v_bias</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span>
            <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span>
        <span class="p">)</span>

        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">rel_pos_bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">"bhic,bijc-&gt;bhij"</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">rel_pos_bias</span><span class="p">)</span>
            <span class="n">attn</span> <span class="o">=</span> <span class="n">attn</span> <span class="o">+</span> <span class="n">bias</span>
        <span class="k">if</span> <span class="n">key_padding_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">key_padding_mask</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
                <span class="ow">or</span> <span class="n">key_padding_mask</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span>
            <span class="p">),</span> <span class="s2">"incorrect mask dtype"</span>
            <span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span>
                <span class="n">key_padding_mask</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:],</span> <span class="n">key_padding_mask</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="n">bias</span><span class="p">[</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span>
                    <span class="n">key_padding_mask</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:],</span> <span class="n">key_padding_mask</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span>
                <span class="p">)</span>
                <span class="o">&lt;</span> <span class="mi">0</span>
            <span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">attn</span> <span class="o">=</span> <span class="n">attn</span> <span class="o">+</span> <span class="n">bias</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">attn</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_drop</span><span class="p">(</span><span class="n">attn</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">attn</span> <span class="o">@</span> <span class="n">v</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">rel_pos_bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">"bhij,bijc-&gt;bihc"</span><span class="p">,</span> <span class="n">attn</span><span class="p">,</span> <span class="n">rel_pos_bias</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">event_length</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj_drop</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span></div>
</div>



<div class="viewcode-block" id="Block">
<a class="viewcode-back" href="../../../../api/graphnet.models.components.layers.html#graphnet.models.components.layers.Block">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">Block</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Transformer block."""</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">mlp_ratio</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">4.0</span><span class="p">,</span>
        <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">attn_drop</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">drop_path</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">init_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">activation</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">,</span>
        <span class="n">norm_layer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">"""Construct 'Block'.</span>

<span class="sd">        Args:</span>
<span class="sd">            input_dim: Dimension of the input tensor.</span>
<span class="sd">            num_heads: Number of attention heads to use in the</span>
<span class="sd">                `MultiheadAttention` layer.</span>
<span class="sd">            mlp_ratio: Ratio of the hidden size of the feedforward network to</span>
<span class="sd">                the input size in the `Mlp` layer.</span>
<span class="sd">            dropout: Dropout probability to use in the `Mlp` layer.</span>
<span class="sd">            attn_drop: Dropout probability to use in the `MultiheadAttention`</span>
<span class="sd">                layer.</span>
<span class="sd">            drop_path: Probability of applying drop path regularization to the</span>
<span class="sd">                output of the layer.</span>
<span class="sd">            init_values: Initial value to use for the `gamma_1` and `gamma_2`</span>
<span class="sd">                parameters if not `None`.</span>
<span class="sd">            activation: Activation function to use in the `Mlp` layer.</span>
<span class="sd">            norm_layer: Normalization layer to use.</span>
<span class="sd">        """</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">norm_layer</span><span class="p">(</span><span class="n">input_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MultiheadAttention</span><span class="p">(</span>
            <span class="n">input_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">attn_drop</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">drop_path</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">DropPath</span><span class="p">(</span><span class="n">drop_path</span><span class="p">)</span> <span class="k">if</span> <span class="n">drop_path</span> <span class="o">&gt;</span> <span class="mf">0.0</span> <span class="k">else</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">norm_layer</span><span class="p">(</span><span class="n">input_dim</span><span class="p">)</span>
        <span class="n">mlp_hidden_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">input_dim</span> <span class="o">*</span> <span class="n">mlp_ratio</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">Mlp</span><span class="p">(</span>
            <span class="n">in_features</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
            <span class="n">hidden_features</span><span class="o">=</span><span class="n">mlp_hidden_dim</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
            <span class="n">dropout_prob</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">init_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gamma_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
                <span class="n">init_values</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">input_dim</span><span class="p">)),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gamma_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
                <span class="n">init_values</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">input_dim</span><span class="p">)),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gamma_1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma_2</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>

<div class="viewcode-block" id="Block.forward">
<a class="viewcode-back" href="../../../../api/graphnet.models.components.layers.html#graphnet.models.components.layers.Block.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">attn_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">key_padding_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Forward pass."""</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma_1</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">xn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_path</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="p">(</span>
                    <span class="n">xn</span><span class="p">,</span>
                    <span class="n">xn</span><span class="p">,</span>
                    <span class="n">xn</span><span class="p">,</span>
                    <span class="n">attn_mask</span><span class="o">=</span><span class="n">attn_mask</span><span class="p">,</span>
                    <span class="n">key_padding_mask</span><span class="o">=</span><span class="n">key_padding_mask</span><span class="p">,</span>
                    <span class="n">need_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_path</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mlp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">xn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_path</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">gamma_1</span>
                <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="p">(</span>
                    <span class="n">xn</span><span class="p">,</span>
                    <span class="n">xn</span><span class="p">,</span>
                    <span class="n">xn</span><span class="p">,</span>
                    <span class="n">attn_mask</span><span class="o">=</span><span class="n">attn_mask</span><span class="p">,</span>
                    <span class="n">key_padding_mask</span><span class="o">=</span><span class="n">key_padding_mask</span><span class="p">,</span>
                    <span class="n">need_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_path</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gamma_2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="k">return</span> <span class="n">x</span></div>
</div>



<div class="viewcode-block" id="GritSparseMHA">
<a class="viewcode-back" href="../../../../api/graphnet.models.components.layers.html#graphnet.models.components.layers.GritSparseMHA">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">GritSparseMHA</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Proposed Attention Computation for GRIT.</span>

<span class="sd">    Original code:</span>
<span class="sd">    https://github.com/LiamMa/GRIT/blob/main/grit/layer/grit_layer.py</span>
<span class="sd">    """</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">out_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">use_bias</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">clamp</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
        <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">activation</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
        <span class="n">edge_enhance</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">"""Construct 'GritSparseMHA'.</span>

<span class="sd">        Args:</span>
<span class="sd">            in_dim: Dimension of the input tensor.</span>
<span class="sd">            out_dim: Dimension of the output tensor.</span>
<span class="sd">            num_heads: Number of attention heads.</span>
<span class="sd">            use_bias: Apply bias the key and value linear layers.</span>
<span class="sd">            clamp: Clamp the absolute value of the attention scores to a value.</span>
<span class="sd">            dropout: Dropout layer probability.</span>
<span class="sd">            activation: Uninstantiated activation function.</span>
<span class="sd">                E.g. `torch.nn.ReLU`</span>
<span class="sd">            edge_enhance: Applies learnable weight matrix with node-pair in</span>
<span class="sd">                output node calculation.</span>
<span class="sd">        """</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span> <span class="o">=</span> <span class="n">out_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">clamp</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">clamp</span><span class="p">)</span> <span class="k">if</span> <span class="n">clamp</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">edge_enhance</span> <span class="o">=</span> <span class="n">edge_enhance</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">Q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">K</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">use_bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">E</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span> <span class="o">*</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">V</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">use_bias</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">E</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">Aw</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Aw</span><span class="p">)</span>

        <span class="c1"># TODO: Better activation function handling -PW</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">activation</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">edge_enhance</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">VeRow</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span><span class="p">),</span>
                <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">VeRow</span><span class="p">)</span>

<div class="viewcode-block" id="GritSparseMHA.forward">
<a class="viewcode-back" href="../../../../api/graphnet.models.components.layers.html#graphnet.models.components.layers.GritSparseMHA.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">Data</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Data</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Forward pass."""</span>
        <span class="n">Q_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">x</span><span class="p">)</span>
        <span class="n">K_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">x</span><span class="p">)</span>
        <span class="n">V_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">x</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">data</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"edge_attr"</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">E</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">E</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">edge_attr</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">E</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="n">Q_x</span> <span class="o">=</span> <span class="n">Q_x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span><span class="p">)</span>
        <span class="n">K_x</span> <span class="o">=</span> <span class="n">K_x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span><span class="p">)</span>
        <span class="n">V_x</span> <span class="o">=</span> <span class="n">V_x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span><span class="p">)</span>

        <span class="c1"># Applying Eq. 2 of the GRIT paper:</span>
        <span class="n">src</span> <span class="o">=</span> <span class="n">K_x</span><span class="p">[</span><span class="n">data</span><span class="o">.</span><span class="n">edge_index</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>  <span class="c1"># (num relative) x num_heads x out_dim</span>
        <span class="n">dest</span> <span class="o">=</span> <span class="n">Q_x</span><span class="p">[</span><span class="n">data</span><span class="o">.</span><span class="n">edge_index</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>  <span class="c1"># (num relative) x num_heads x out_dim</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">src</span> <span class="o">+</span> <span class="n">dest</span>  <span class="c1"># element-wise multiplication</span>
        <span class="k">if</span> <span class="n">E</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">E</span> <span class="o">=</span> <span class="n">E</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">E_w</span><span class="p">,</span> <span class="n">E_b</span> <span class="o">=</span> <span class="n">E</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span><span class="p">],</span> <span class="n">E</span><span class="p">[:,</span> <span class="p">:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span> <span class="p">:]</span>
            <span class="n">score</span> <span class="o">=</span> <span class="n">score</span> <span class="o">*</span> <span class="n">E_w</span>
            <span class="n">score</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">score</span><span class="p">))</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="o">-</span><span class="n">score</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">score</span> <span class="o">=</span> <span class="n">score</span> <span class="o">+</span> <span class="n">E_b</span>

        <span class="n">score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
        <span class="n">e_t</span> <span class="o">=</span> <span class="n">score</span>  <span class="c1"># ehat_ij</span>

        <span class="c1"># Output edge</span>
        <span class="k">if</span> <span class="n">E</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">wE</span> <span class="o">=</span> <span class="n">score</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Complete attention calculation</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">"ehd, dhc-&gt;ehc"</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Aw</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">clamp</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">score</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">score</span><span class="p">,</span> <span class="nb">min</span><span class="o">=-</span><span class="bp">self</span><span class="o">.</span><span class="n">clamp</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">clamp</span><span class="p">)</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">score</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">edge_index</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span>
        <span class="p">)</span>  <span class="c1"># (num relative) x num_heads x 1</span>
        <span class="n">score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>

        <span class="c1"># Aggregate with Attn-Score</span>
        <span class="n">V_x_weighted</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">V_x</span><span class="p">[</span><span class="n">data</span><span class="o">.</span><span class="n">edge_index</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">*</span> <span class="n">score</span>
        <span class="p">)</span>  <span class="c1"># (num relative) x num_heads x out_dim</span>
        <span class="n">wV</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span>
            <span class="n">V_x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">score</span><span class="o">.</span><span class="n">dtype</span>
        <span class="p">)</span>  <span class="c1"># (num nodes in batch) x num_heads x out_dim</span>
        <span class="n">scatter</span><span class="p">(</span><span class="n">V_x_weighted</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">edge_index</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">wV</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="s2">"add"</span><span class="p">)</span>

        <span class="c1"># Adds the second term (W_Ev ehhat_ij) in the last line of Eq. 2</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">edge_enhance</span> <span class="ow">and</span> <span class="n">E</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">rowV</span> <span class="o">=</span> <span class="n">scatter</span><span class="p">(</span>
                <span class="n">e_t</span> <span class="o">*</span> <span class="n">score</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">edge_index</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="s2">"add"</span>
            <span class="p">)</span>
            <span class="n">rowV</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">"nhd, dhc -&gt; nhc"</span><span class="p">,</span> <span class="n">rowV</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">VeRow</span><span class="p">)</span>
            <span class="n">wV</span> <span class="o">=</span> <span class="n">wV</span> <span class="o">+</span> <span class="n">rowV</span>

        <span class="k">return</span> <span class="n">wV</span><span class="p">,</span> <span class="n">wE</span></div>
</div>



<div class="viewcode-block" id="GritTransformerLayer">
<a class="viewcode-back" href="../../../../api/graphnet.models.components.layers.html#graphnet.models.components.layers.GritTransformerLayer">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">GritTransformerLayer</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Proposed Transformer Layer for GRIT.</span>

<span class="sd">    Original code:</span>
<span class="sd">    https://github.com/LiamMa/GRIT/blob/main/grit/layer/grit_layer.py</span>
<span class="sd">    """</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">out_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">norm</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">,</span>
        <span class="n">residual</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">deg_scaler</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">activation</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
        <span class="n">norm_edges</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">update_edges</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">batch_norm_momentum</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">batch_norm_runner</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">rezero</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">enable_edge_transform</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">attn_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">attn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">attn_clamp</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
        <span class="n">attn_activation</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
        <span class="n">attn_edge_enhance</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">"""Construct 'GritTransformerLayer'.</span>

<span class="sd">        Args:</span>
<span class="sd">            in_dim: Dimension of the input tensor.</span>
<span class="sd">            out_dim: Dimension of theo output tensor.</span>
<span class="sd">            num_heads: Number of attention heads.</span>
<span class="sd">            dropout: Dropout layer probability.</span>
<span class="sd">            norm: Uninstantiated normalization layer.</span>
<span class="sd">                Must be either `torch.nn.BatchNorm1d` or `torch.nn.LayerNorm`.</span>
<span class="sd">            residual: Apply residual connections.</span>
<span class="sd">            deg_scaler: Apply degree scaling after MHA.</span>
<span class="sd">            activation: Uninstantiated activation function.</span>
<span class="sd">                E.g. `torch.nn.ReLU`</span>
<span class="sd">            norm_edges: Apply normalization to edges.</span>
<span class="sd">            update_edges: Update edges after layer.</span>
<span class="sd">            batch_norm_momentum: Momentum of batch normalization.</span>
<span class="sd">            batch_norm_runner: Track running stats of batch normalization.</span>
<span class="sd">            rezero: Apply learnable scaling parameters.</span>
<span class="sd">            enable_edge_transform: Apply a FC to edges at the start</span>
<span class="sd">                of the layer.</span>
<span class="sd">            attn_bias: Add bias to keys and values in MHA block.</span>
<span class="sd">            attn_dropout: Attention droput.</span>
<span class="sd">            attn_clamp: Clamp absolute value of attention scores to a value.</span>
<span class="sd">            attn_activation: Uninstantiated activation function for MHA block.</span>
<span class="sd">                E.g. `torch.nn.ReLU`</span>
<span class="sd">            attn_edge_enhance: Applies learnable weight matrix with node-pair</span>
<span class="sd">                in output node calculation in MHA block.</span>
<span class="sd">        """</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="n">in_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">=</span> <span class="n">out_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_dim</span> <span class="o">=</span> <span class="n">in_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_dim</span> <span class="o">=</span> <span class="n">out_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">residual</span> <span class="o">=</span> <span class="n">residual</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_edges</span> <span class="o">=</span> <span class="n">update_edges</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm_momentum</span> <span class="o">=</span> <span class="n">batch_norm_momentum</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm_runner</span> <span class="o">=</span> <span class="n">batch_norm_runner</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rezero</span> <span class="o">=</span> <span class="n">rezero</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">deg_scaler</span> <span class="o">=</span> <span class="n">deg_scaler</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">activation</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">GritSparseMHA</span><span class="p">(</span>
            <span class="n">in_dim</span><span class="o">=</span><span class="n">in_dim</span><span class="p">,</span>
            <span class="n">out_dim</span><span class="o">=</span><span class="n">out_dim</span> <span class="o">//</span> <span class="n">num_heads</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
            <span class="n">use_bias</span><span class="o">=</span><span class="n">attn_bias</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">attn_dropout</span><span class="p">,</span>
            <span class="n">clamp</span><span class="o">=</span><span class="n">attn_clamp</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="n">attn_activation</span><span class="p">,</span>
            <span class="n">edge_enhance</span><span class="o">=</span><span class="n">attn_edge_enhance</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">fc1_x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">out_dim</span> <span class="o">//</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">enable_edge_transform</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">fc1_e</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">out_dim</span> <span class="o">//</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">fc1_e</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">deg_scaler</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">deg_coef</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">out_dim</span> <span class="o">//</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">deg_coef</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">norm</span> <span class="o">==</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">norm1_x</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="n">out_dim</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">norm1_e</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">out_dim</span><span class="p">)</span> <span class="k">if</span> <span class="n">norm_edges</span> <span class="k">else</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>
        <span class="k">elif</span> <span class="n">norm</span> <span class="o">==</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">norm1_x</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span>
                <span class="n">out_dim</span><span class="p">,</span>
                <span class="n">track_running_stats</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_norm_runner</span><span class="p">,</span>
                <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
                <span class="n">momentum</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_norm_momentum</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">norm1_e</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">norm</span><span class="p">(</span>
                    <span class="n">out_dim</span><span class="p">,</span>
                    <span class="n">track_running_stats</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_norm_runner</span><span class="p">,</span>
                    <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
                    <span class="n">momentum</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_norm_momentum</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="n">norm_edges</span>
                <span class="k">else</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">"GritTransformerLayer normalization layer must be 'LayerNorm' </span><span class="se">\</span>
<span class="s2">                    or 'BatchNorm1d'!"</span>
            <span class="p">)</span>

        <span class="c1"># FFN for x</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">FFN_x_layer1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">out_dim</span><span class="p">,</span> <span class="n">out_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">FFN_x_layer2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">out_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">norm</span> <span class="o">==</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">norm2_x</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="n">out_dim</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">norm</span> <span class="o">==</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">norm2_x</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span>
                <span class="n">out_dim</span><span class="p">,</span>
                <span class="n">track_running_stats</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_norm_runner</span><span class="p">,</span>
                <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
                <span class="n">momentum</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_norm_momentum</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rezero</span><span class="p">:</span>  <span class="c1"># Learnable scaling parameters</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">alpha1_x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">alpha2_x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">alpha1_e</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>  <span class="c1"># Post-attention dropout on x</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>  <span class="c1"># Post-attention dropout on e</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>  <span class="c1"># Post-FFN dropout on x</span>

<div class="viewcode-block" id="GritTransformerLayer.forward">
<a class="viewcode-back" href="../../../../api/graphnet.models.components.layers.html#graphnet.models.components.layers.GritTransformerLayer.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">Data</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Data</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Forward pass."""</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">x</span>
        <span class="n">num_nodes</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">num_nodes</span>
        <span class="n">log_deg</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span>
            <span class="n">degree</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">edge_index</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">num_nodes</span><span class="o">=</span><span class="n">num_nodes</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="o">+</span> <span class="mi">1</span>
        <span class="p">)</span>
        <span class="n">log_deg</span> <span class="o">=</span> <span class="n">log_deg</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">num_nodes</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">x_attn_residual</span> <span class="o">=</span> <span class="n">x</span>  <span class="c1"># for first residual connection</span>
        <span class="n">e_values_in</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"edge_attr"</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">e</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Attention outputs</span>
        <span class="n">x_attn_out</span><span class="p">,</span> <span class="n">e_attn_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">x_attn_out</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">num_nodes</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Apply degree scaler if enabled</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">deg_scaler</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span> <span class="o">*</span> <span class="n">log_deg</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">deg_coef</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">e_attn_out</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">e</span> <span class="o">=</span> <span class="n">e_attn_out</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">e</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
            <span class="n">e</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1_e</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rezero</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha1_x</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x_attn_residual</span> <span class="o">+</span> <span class="n">x</span>

            <span class="k">if</span> <span class="n">e</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rezero</span><span class="p">:</span>
                    <span class="n">e</span> <span class="o">=</span> <span class="n">e</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha1_e</span>
                <span class="n">e</span> <span class="o">=</span> <span class="n">e</span> <span class="o">+</span> <span class="n">e_values_in</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">e</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">e</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1_e</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>

        <span class="c1"># FFN for x</span>
        <span class="n">x_ffn_residual</span> <span class="o">=</span> <span class="n">x</span>  <span class="c1"># Residual over the FFN</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">FFN_x_layer1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">FFN_x_layer2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rezero</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha2_x</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x_ffn_residual</span> <span class="o">+</span> <span class="n">x</span>  <span class="c1"># residual connection</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2_x</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">data</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_edges</span><span class="p">:</span>
            <span class="n">data</span><span class="o">.</span><span class="n">edge_attr</span> <span class="o">=</span> <span class="n">e</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">data</span><span class="o">.</span><span class="n">edge_attr</span> <span class="o">=</span> <span class="n">e_values_in</span>

        <span class="k">return</span> <span class="n">data</span></div>
</div>



<span class="c1"># TODO: This is a prediction head... we probably want only the graph stuff here</span>
<span class="c1"># and let the Tasks handle the last layer. -PW</span>
<div class="viewcode-block" id="SANGraphHead">
<a class="viewcode-back" href="../../../../api/graphnet.models.components.layers.html#graphnet.models.components.layers.SANGraphHead">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">SANGraphHead</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""SAN prediction head for graph prediction tasks.</span>

<span class="sd">    Original code:</span>
<span class="sd">    https://github.com/LiamMa/GRIT/blob/main/grit/head/san_graph.py</span>
<span class="sd">    """</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">dim_in</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">dim_out</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">L</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
        <span class="n">activation</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
        <span class="n">pooling</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">"mean"</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">"""Construct `SANGraphHead`.</span>

<span class="sd">        Args:</span>
<span class="sd">            dim_in: Input dimension.</span>
<span class="sd">            dim_out: Output dimension.</span>
<span class="sd">            L: Number of hidden layers.</span>
<span class="sd">            activation: Uninstantiated activation function.</span>
<span class="sd">                E.g. `torch.nn.ReLU`</span>
<span class="sd">            pooling: Node-wise pooling operation. Either "mean" or "add".</span>
<span class="sd">        """</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">pooling</span> <span class="o">==</span> <span class="s2">"mean"</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pooling_fun</span> <span class="o">=</span> <span class="n">global_mean_pool</span>
        <span class="k">elif</span> <span class="n">pooling</span> <span class="o">==</span> <span class="s2">"add"</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pooling_fun</span> <span class="o">=</span> <span class="n">global_add_pool</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">"Currently supports only 'add' or 'mean'."</span><span class="p">)</span>

        <span class="n">fc_layers</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_in</span> <span class="o">//</span> <span class="mi">2</span><span class="o">**</span><span class="n">n</span><span class="p">,</span> <span class="n">dim_in</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>
        <span class="p">]</span>
        <span class="k">assert</span> <span class="n">dim_in</span> <span class="o">//</span> <span class="mi">2</span><span class="o">**</span><span class="n">L</span> <span class="o">&gt;=</span> <span class="n">dim_out</span><span class="p">,</span> <span class="s2">"Too much dim reduction!"</span>
        <span class="n">fc_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim_in</span> <span class="o">//</span> <span class="mi">2</span><span class="o">**</span><span class="n">L</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span><span class="n">fc_layers</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">L</span> <span class="o">=</span> <span class="n">L</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">activation</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim_out</span> <span class="o">=</span> <span class="n">dim_out</span>

<div class="viewcode-block" id="SANGraphHead.forward">
<a class="viewcode-back" href="../../../../api/graphnet.models.components.layers.html#graphnet.models.components.layers.SANGraphHead.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">Data</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Forward Pass."""</span>
        <span class="n">graph_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pooling_fun</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">batch</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">L</span><span class="p">):</span>
            <span class="n">graph_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_layers</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">graph_emb</span><span class="p">)</span>
            <span class="n">graph_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">graph_emb</span><span class="p">)</span>
        <span class="n">graph_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_layers</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">L</span><span class="p">](</span><span class="n">graph_emb</span><span class="p">)</span>
        <span class="c1"># Original code applied a final linear layer to project to dim_out,</span>
        <span class="c1"># but we will let the Task layer do that.</span>
        <span class="k">return</span> <span class="n">graph_emb</span></div>
</div>

</pre></div>

          </article>
        </div>
      </div>
    </main>
  </div>
  <footer class="md-footer">
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
          
          
        </a>
        
      </nav>
    </div>
    <div class="md-footer-meta md-typeset">
      <div class="md-footer-meta__inner md-grid">
        <div class="md-footer-copyright">
          <div class="md-footer-copyright__highlight">
              &#169; Copyright 2021-2025, GraphNeT team.
              
          </div>
            Created using
            <a href="http://www.sphinx-doc.org/">Sphinx</a> 8.1.3.
             and
            <a href="https://github.com/bashtage/sphinx-material/">Material for
              Sphinx</a>
        </div>
      </div>
    </div>
  </footer>
  <script src="../../../../_static/javascripts/application.js"></script>
  <script>app.initialize({version: "1.0.4", url: {base: ".."}})</script>
  </body>
</html>