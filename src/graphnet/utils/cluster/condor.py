"""
Tools for submitting to a cluster via the condor system.
Also supports DAGMan.

These tools are generic to any condor system, e.g. do not assume IceCube, NPX, etc.

These can be used standalone, or by the more sophisticated job management system 
defined in `cluster.py` and `job.py`.

Tom Stuttard
"""
from __future__ import print_function
from __future__ import absolute_import
from __future__ import division
from __future__ import unicode_literals

from builtins import open
from builtins import int
from builtins import str
from builtins import object
from future import standard_library

standard_library.install_aliases()
import os

from graphnet.utilities.cluster.filesys_tools import make_dir, is_executable
from graphnet.utilities.cluster.unix_tools import tail

from graphnet.utilities.cluster.job import JOB_INDEX_FMT, JOB_NAME_FMT

#
# Globals
#

CONDOR_SUBMIT_EXE = "condor_submit_dag"

#
# Submission
#


def percentage(numerator, denominator):
    return 100.0 * fraction(numerator, denominator)


def create_condor_submit_file(
    file_path,
    memory_MB=1000,
    disk_space_MB=1000,
    num_cpus=1,
    num_gpus=0,
    export_env=False,
    require_cvmfs=False,
    require_avx=False,
    require_cuda=False,
    require_sl7=False,  # Scientific Linux 7 (+ variants)
    choose_sites=None,
    exclude_sites=None,
    user_proxy=False,
    wall_time_hr=None,
    accounting_group=None,
):
    """
    Create a condor submit file.
    Assumes it will be run as a DAG, using a DAGMan scriped generated using `create_dagman_submit_file` (below).
    """

    # Check inputs
    assert not (
        choose_sites and exclude_sites
    ), "Cannot specify both `choose_sites` and `exclude_sites`"

    # Make paths absolute
    file_path = os.path.abspath(file_path)

    # Check parent directory exists
    base_dir = os.path.dirname(file_path)
    make_dir(base_dir)

    # Make file
    with open(file_path, "w") as submit_file:

        # Write a header
        submit_file.write(
            "# Autogenerated using fridge/utils/cluster/condor.create_condor_submit_file\n"
        )

        # Write executable line
        submit_file.write("executable = $(Exe)\n")

        # Write arguments line
        submit_file.write("arguments = $(ExeArgs)\n")

        # Write output streams lines
        submit_file.write("output = $(OutFile)\n")
        submit_file.write("error = $(ErrFile)\n")
        submit_file.write("log = $(LogFile)\n")

        # Set the working directory
        submit_file.write(
            "initialdir = $(WorkingDir)\n"
        )  # TODO Not sure I actually set this anywhere

        # Write notification line
        submit_file.write("notification = never\n")

        # Memory/disk request
        submit_file.write("request_memory = %iMB\n" % memory_MB)
        submit_file.write("request_disk = %iMB\n" % disk_space_MB)

        # CPU request
        submit_file.write("request_cpus = %i\n" % num_cpus)

        # GPU request
        # Usage guide: https://wiki.icecube.wisc.edu/index.php/Condor#GPU_usage
        # Note that should not include `request_gpus = 0` if don't need any, as on the IceCube grid this
        # results in the following property`(TARGET.gpus >= Requestgpus)` being set, which means that
        # clusters not delcaring the `gpus` classad won't be allocated my jobs, even though I don't need
        # GPUs (thanks vbrik for the info)
        if num_gpus > 0:
            submit_file.write("request_gpus = %i\n" % num_gpus)

        # Export the environment of the submitter if the user requested it
        # WARNING : This can be dangerous, and is not recommended (particularly if using GPUs)
        if export_env:
            submit_file.write("getenv = True\n")

        # Copy local files back across in the end
        submit_file.write(
            "should_transfer_files = YES\n"
        )  # TODO Think more about this, there are various option

        # Build a requirements list
        requirements = []
        if require_cvmfs:
            requirements.append("HAS_CVMFS_icecube_opensciencegrid_org")
        if require_avx:
            requirements.append("Has_AVX")
        if require_cuda:
            requirements.append("CUDACapability")
        if require_sl7:
            requirements.append(
                '( OpSysAndVer == "SL7" || OpSysAndVer == "CentOS7" )'
            )
        if (choose_sites is not None) and (len(choose_sites) > 0):
            requirements.append(
                "( "
                + (
                    " || ".join(
                        [
                            '(GLIDEIN_Site == "%s")' % site
                            for site in choose_sites
                        ]
                    )
                    + " )"
                )
            )
        if (exclude_sites is not None) and (len(exclude_sites) > 0):
            requirements.append(
                "( "
                + (
                    " && ".join(
                        [
                            '(GLIDEIN_Site =!= "%s")' % site
                            for site in exclude_sites
                        ]
                    )
                    + " )"
                )
            )
        if len(requirements) > 0:
            submit_file.write(
                "Requirements = ( %s )\n" % (" && ".join(requirements))
            )

        # Handle user certification for remote sites where this is required
        if user_proxy:
            submit_file.write("use_x509userproxy=true\n")

        # Define a walltime
        if wall_time_hr is not None:
            submit_file.write(
                "+OriginalTime = %i\n" % (wall_time_hr * 3600)
            )  # Madison ClassAd. Unit is [s]. Not sure how many sites support this, but works at least for MSU and doesn't seem to break anything elsewhere
            # TODO are there other ways to specify a walltime? e.g. other grid sites
            # TODO NPX (which has special queues)

        # Add accounting group if requested
        if accounting_group is not None:
            submit_file.write(
                '+AccountingGroup = "%s.$ENV(USER)"\n' % accounting_group
            )

        # Write queue line
        submit_file.write("queue\n")

        print(("Condor submit file written : %s" % (file_path)))
        return file_path


# Create a DAGMan submit file
def create_dagman_submit_file(
    submit_dir,
    log_dir,
    dagman_file_name,
    condor_file_name,
    jobs,
    **submit_file_kw,
):

    #
    # Handle inputs
    #

    # Check the submission file dir exists
    submit_dir = os.path.expandvars(os.path.expanduser(submit_dir))
    submit_dir = os.path.abspath(submit_dir)
    assert os.path.isdir(submit_dir), (
        'Cannot create DAGMan file, submission file directory "%s" does not exist'
        % submit_dir
    )

    # Form paths for submit files to be generated
    dagman_file_path = os.path.abspath(
        os.path.join(submit_dir, dagman_file_name)
    )
    condor_file_path = os.path.abspath(
        os.path.join(submit_dir, condor_file_name)
    )

    #
    # Create submit files
    #

    # Create the submit file this DAGMan file will use
    create_condor_submit_file(condor_file_path, **submit_file_kw)

    # Make file
    with open(dagman_file_path, "w") as dagman_file:

        # Write a header
        dagman_file.write(
            "# Autogenerated using fridge/utils/cluster/condor.create_dagman_submit_file\n"
        )

        # Loop over jobs provided (each one is a list of args)
        for job in jobs:

            # Write job line
            job_name = JOB_NAME_FMT % job.index
            dagman_file.write("JOB %s %s\n" % (job_name, condor_file_path))

            # Break command up in to exe/args
            # Depends on whether an env shell is provided, and whether a job wrapper is used
            if job.wrapper_script is None:
                assert (
                    len(job.commands) == 1
                ), "No wrapper mode only supports single commands, no bunching"
                exe_path = job.commands[0].command
                # exe_args = ""
            else:
                exe_path = job.wrapper_script
                # exe_args = ""

            # Check executable exists and is executable
            assert os.path.isfile(exe_path), (
                "Executable file does not exist : %s" % exe_path
            )
            assert is_executable(exe_path), (
                "Executable file is not executable (change the permissions) : %s"
                % exe_path
            )

            # Define log file
            # This can hammer a file system, so normally want this to be scratch
            log_file = os.path.join(
                log_dir, JOB_INDEX_FMT % job.index + ".log"
            )

            # Define generic args used for all jobs
            arg_string = 'JobIndex="%s"' % (JOB_INDEX_FMT % job.index)
            arg_string += ' Exe="%s"' % (exe_path)
            # arg_string += " ExeArgs=\"%s\"" % (exe_args)
            arg_string += ' OutFile="%s"' % (job.out_file)
            arg_string += ' ErrFile="%s"' % (job.err_file)
            arg_string += ' LogFile="%s"' % (log_file)

            # Write the args for the job
            dagman_file.write("VARS %s %s\n" % (job_name, arg_string))

    # Done
    print(("DAGMan submit file written : %s" % (dagman_file_path)))
    return dagman_file_path, condor_file_path


#
# Parsing metrics
#

# Interpret the DAG status codes (from https://research.cs.wisc.edu/htcondor/manual/latest/2_10DAGMan_Applications.html)
def interpret_dag_status(dag_status):
    if dag_status == 0:
        return "OK"
    elif dag_status == 1:
        return "Other error"
    elif dag_status == 2:
        return "One or more nodes failed"
    elif dag_status == 3:
        return "DAG aborted (ABORT-DAG-ON)"
    elif dag_status == 4:
        return "DAG removed (condor_rm)"
    elif dag_status == 5:
        return "DAG cycle found"
    elif dag_status == 6:
        return "DAG halted"
    else:
        raise Exception("Unknown DAG status : %i" % (dag_status))


# Class for storing DAGMan jobs bunch metrics from the metrics file json node
class DAGManMetrics(object):

    # Members of DAGManMetrics main class
    def __init__(self, date=None):
        self.date = date
        self.submit_fileInfo = None
        self.outputFileInfo = None
        self.metricsFileInfo = None
        self.jobsMetrics = list()  # TOOD OrderedDict with num?

    def parseSubmitFile(self, dagmanSubmitFilePath):
        self.submit_fileInfo = DAGManMetricsSubmitFileInfo(
            dagmanSubmitFilePath
        )

    def parseOutputFile(self, dagmanOutputFilePath):
        self.outputFileInfo = DAGManMetricsOutputFileInfo(dagmanOutputFilePath)

    def parseMetricsFile(self, metricsJsonFilePath):
        self.metricsFileInfo = DAGManMetricsMetricsFileInfo(
            metricsJsonFilePath
        )

    def fullyParsed(self):
        return (
            self.submit_fileInfo != None
            and self.submit_fileInfo.fullyParsed()
            and self.outputFileInfo != None
            and self.outputFileInfo.fullyParsed()
            and self.metricsFileInfo != None
            and self.metricsFileInfo.fullyParsed()
        )

    def __str__(self):
        returnString = "DAGMan submission :\n"
        if self.date != None:
            returnString += "  Date                  : %s\n" % (self.date)

        if self.submit_fileInfo != None:
            returnString += "  Submit file info :\n"
            returnString += "    Num jobs            : %i\n" % (
                self.submit_fileInfo.numJobs
            )
        else:
            returnString += "  [Submit file not parsed]\n"

        if self.outputFileInfo != None:
            returnString += "  Output file info :\n"
            returnString += "    Return code          : %s\n" % (
                str(self.outputFileInfo.returnCode)
                if self.outputFileInfo.returnCode != None
                else "[Not parsed]"
            )
        else:
            returnString += "  [Output file not parsed]\n"

        if self.metricsFileInfo != None:
            returnString += "  Metrics file info :\n"
            returnString += "    Num jobs             : %i\n" % (
                self.metricsFileInfo.numJobs
            )
            returnString += "    Num jobs succeeded   : %i (%0.3f%%)\n" % (
                self.metricsFileInfo.numJobsSucceeded,
                percentage(
                    self.metricsFileInfo.numJobsSucceeded,
                    self.metricsFileInfo.numJobs,
                ),
            )
            returnString += "    Num jobs failed      : %i (%0.3f%%)\n" % (
                self.metricsFileInfo.numJobsFailed,
                percentage(
                    self.metricsFileInfo.numJobsFailed,
                    self.metricsFileInfo.numJobs,
                ),
            )
            returnString += "    Start time           : %s\n" % (
                self.metricsFileInfo.start_time
            )
            returnString += "    End time             : %s\n" % (
                self.metricsFileInfo.end_time
            )
            returnString += "    Time taken in total  : %s\n" % (
                time.human_time(self.metricsFileInfo.duration)
            )
            returnString += "    DAG status           : %s\n" % (
                interpretDAGStatus(self.metricsFileInfo.dag_status)
            )
        else:
            returnString += "  [Metrics file not parsed]\n"

        return returnString


# Subclass containing submit file information
class DAGManMetricsSubmitFileInfo(object):
    def __init__(self, dagmanSubmitFilePath):
        self.parse(dagmanSubmitFilePath)

    def fullyParsed(self):
        return True  # No conditional parsing for this file at this time

    def parse(self, dagmanSubmitFilePath):

        # Check submit exists, open it, and read it into a string
        if os.path.exists(dagmanSubmitFilePath):
            with open(dagmanSubmitFilePath) as submit_file:
                submit_fileText = submit_file.read()

                # Count the instances of the "JOB" ketword in the string
                self.numJobs = submit_fileText.count("JOB")

        else:
            raise Exception(
                'DAGMan submit file "%s" does not exist' % dagmanSubmitFilePath
            )


# Subclass containing metrics file information
class DAGManMetricsMetricsFileInfo(object):
    def __init__(self, metricsJsonFilePath):
        self.parse(metricsJsonFilePath)

    def fullyParsed(self):
        return True  # No conditional parsing for this file at this time

    def parse(self, metricsJsonFilePath):

        # Open the JSON metrics file and parse using a json parser
        if os.path.exists(metricsJsonFilePath):
            with open(metricsJsonFilePath) as dagmanMetricsFile:

                metricsNode = json.load(dagmanMetricsFile)
                if metricsNode:

                    # Read individual details from the metrics file
                    self.numJobs = metricsNode["jobs"]
                    self.numJobsSucceeded = metricsNode["jobs_succeeded"]
                    self.numJobsFailed = metricsNode["jobs_failed"]
                    self.start_time = metricsNode["start_time"]
                    self.end_time = metricsNode["end_time"]
                    self.duration = metricsNode["duration"]
                    # self.jobsCombinedDuration = metricsNode['total_job_time'] #TODO replace once DAGman actually writes this variable, currently always 0
                    self.dag_status = metricsNode["dag_status"]

                else:
                    raise Exception(
                        'DAGMan metrics file "%s" cannot be parsed as a JSON file'
                        % metricsJsonFilePath
                    )

        else:
            raise Exception(
                'DAGMan metrics file "%s" does not exist' % metricsJsonFilePath
            )


# Subclass containing .out file information
class DAGManMetricsOutputFileInfo(object):
    def __init__(self, dagmanOutputFilePath):
        self.returnCode = None
        self.parse(dagmanOutputFilePath)

    def parse(self, dagmanOutputFilePath):  # TODO parse more of this file

        # Check submit exists, open it, and read it into a string
        if os.path.exists(dagmanOutputFilePath):

            # Parse the return status code
            self.returnCode = self.getReturnCode(dagmanOutputFilePath)

        else:
            raise Exception(
                'DAGMan .out file "%s" does not exist' % dagmanOutputFilePath
            )

    def fullyParsed(self):
        return self.returnCode != None

    def getReturnCode(self, dagmanOutputFilePath):

        # Get the last line of the file (which has the status, assuming it has completed)
        statusLines = tail(dagmanOutputFilePath, 1)

        # Check got the one line I expected
        if len(statusLines) == 1:

            statusLine = statusLines[0]

            # Parse status from this line
            statusKey = "EXITING WITH STATUS"
            if statusKey in statusLine:

                # Extract the status key and return it
                statusKeyStartIndex = statusLine.find(statusKey)
                statusCodeStartIndex = (
                    statusKeyStartIndex + len(statusKey) + 1
                )  # +1 for the space after it
                if len(statusLine) > (statusCodeStartIndex + 1):
                    return int(
                        statusLine[
                            statusCodeStartIndex : statusCodeStartIndex + 1
                        ]
                    )

        # If here, failed in parsing return code (maybe DAG is still running, or died in an uncontrolled way)
        return None


# Class for storing metrics for a given condor job, as parsed from the log file
class CondorJobMetrics(object):

    # Members of CondorJobMetrics main class
    def __init__(self, num=-1):
        self.number = num
        self.logFileInfo = None
        self.outFileInfo = None

    def parseLogFile(self, logFilePath):
        self.logFileInfo = CondorJobMetricsLogFileInfo(logFilePath)

    def parseOutFile(self, outFilePath):
        self.outFileInfo = CondorJobMetricsOutFileInfo(outFilePath)

    def fullyParsed(self):
        return (
            self.logFileInfo != None
            and self.logFileInfo.fullyParsed()
            and self.outFileInfo != None
            and self.outFileInfo.fullyParsed()
        )

    def __str__(self):
        returnString = "Job %s:\n" % (
            "%i" % self.number if self.number > -1 else ""
        )

        if self.logFileInfo != None:
            returnString += "  Log file info :\n"
            returnString += "    Submit time        : %s\n" % (
                self.logFileInfo.submitTime
                if self.logFileInfo.submitTime != None
                else "[Not parsed]"
            )
            returnString += "    Execute time       : %s\n" % (
                self.logFileInfo.executeTime
                if self.logFileInfo.executeTime != None
                else "[Not parsed]"
            )
            returnString += "    Terminate time     : %s\n" % (
                self.logFileInfo.terminateTime
                if self.logFileInfo.terminateTime != None
                else "[Not parsed]"
            )
            returnString += "    Execution duration : %s\n" % (
                self.logFileInfo.getExecuteDuration()
                if self.logFileInfo.getExecuteDuration() != None
                else "[Not parsed]"
            )
            returnString += "    Memory used        : %s\n" % (
                "%i [MB]" % self.logFileInfo.memoryUsedMB
                if self.logFileInfo.memoryUsedMB != None
                else "[Not parsed]"
            )
            returnString += "    Memory requested   : %s\n" % (
                "%i [MB]" % self.logFileInfo.memoryRequestedMB
                if self.logFileInfo.memoryRequestedMB != None
                else "[Not parsed]"
            )
            returnString += "    Memory allocated   : %s\n" % (
                "%i [MB]" % self.logFileInfo.memoryAllocatedMB
                if self.logFileInfo.memoryAllocatedMB != None
                else "[Not parsed]"
            )
            returnString += "    Return code        : %s\n" % (
                str(self.logFileInfo.returnCode)
                if self.logFileInfo.returnCode != None
                else "[Not parsed]"
            )
        else:
            returnString += "  [Log file not parsed]\n"

        if self.outFileInfo != None:
            returnString += "  Out file info :\n"
            returnString += "    Num sub jobs       : %s\n" % (
                self.outFileInfo.numCommands
                if self.outFileInfo.numCommands != None
                else "[Not parsed]"
            )
        else:
            returnString += "  [Out file not parsed]\n"

        return returnString


# Subclass to be filled with parsed out file
class CondorJobMetricsOutFileInfo(object):
    def __init__(self, outFilePath):
        self.numCommands = None
        self.parse(outFilePath)

    def fullyParsed(self):
        return self.numCommands != None

    def parse(self, outFilePath):

        # Check file exists, open it, and read into a string
        if os.path.exists(outFilePath):
            self.outFilePath = outFilePath
            with open(self.outFilePath) as outFile:
                outFileText = outFile.read()

                # Parse num commands
                self.numCommands = self.getNumCommandsFromFile(outFileText)

    def getNumCommandsFromFile(self, outFileText):

        # Split text into lines
        lines = outFileText.split("\n")

        # Find num commands line
        key = "JOB INFO : Num commands = "
        for line in lines:
            keyPos = line.find(key)
            if keyPos > -1:

                # Num commands is rest of line after the key
                return int(line[keyPos + len(key) :])

        # If here, didn't manage to parse
        return None


# Subclass to be filled with parsed log file
class CondorJobMetricsLogFileInfo(object):
    def __init__(self, logFilePath):
        self.submitTime = None
        self.executeTime = None
        self.terminateTime = None
        self.memoryUsedMB = None
        self.memoryRequestedMB = None
        self.memoryAllocatedMB = None
        self.returnCode = None
        self.held = None
        self.aborted = None
        self.timeLimitExceeded = None
        self.parse(logFilePath)

    def fullyParsed(self):
        return (
            self.submitTime != None
            and self.executeTime != None
            and self.terminateTime != None
            and self.memoryUsedMB != None
            and self.memoryRequestedMB != None
            and self.memoryAllocatedMB != None
            and self.returnCode != None
        )

    def parse(self, logFilePath):

        # Check file exists, open it, and read into a string
        if os.path.exists(logFilePath):
            self.logFilePath = logFilePath
            with open(self.logFilePath) as logFile:

                logFileText = logFile.read()

                # Fill members from info in log file
                # Note that if job file is complete, not all members will be parsed (set to None)

                # self.job_number

                # Parse various times from file
                self.submitTime = self.getTimeFromLineInFile("Job submitted")
                self.executeTime = self.getTimeFromLineInFile(
                    "Job executing on host"
                )
                self.terminateTime = self.getTimeFromLineInFile(
                    "Job terminated"
                )

                # Handle corner case of job running over a year
                # This is because job log file doesn't include year, so we have to guess it from file modification date
                if self.terminateTime and self.executeTime:
                    if (
                        self.executeTime > self.terminateTime
                    ):  # e.g. if start after end
                        self.terminateTime += datetime.timedelta(
                            days=365.25
                        )  # Not perfect but good enough for metrics

                (
                    self.memoryUsedMB,
                    self.memoryRequestedMB,
                    self.memoryAllocatedMB,
                ) = self.getMemoryUsageFromFile(logFileText)

                self.returnCode = self.getReturnCodeFromFile(logFileText)

                # Check if job was held
                self.aborted = "Job was held" in logFileText

                # Check if job was aborted
                self.aborted = "Job was aborted by the user" in logFileText

                # Check if time limit breached
                self.timeLimitExceeded = (
                    "Execution time limit exceeded" in logFileText
                )

        else:
            raise Exception("Job log file does not exists")

    def getExecuteDuration(self):
        if self.terminateTime and self.executeTime:
            return self.terminateTime - self.executeTime
        else:
            return None

    def getTimeFromLineInFile(self, key):

        # Open the log file
        with open(self.logFilePath) as logFile:

            # Get log file as text (is not too big)
            logFileText = logFile.read()

            # Get time stamp string from file text
            timeStampLength = 14
            pos = logFileText.find(key)
            if pos > -1:  # check found key
                if pos > timeStampLength:
                    timeStamp = logFileText[pos - timeStampLength : pos]

                    # Time stamp from condor job log files is annoyingly missing the year, guess it from the file modification date
                    # print time.gmtime(os.path.getmtime(self.logFilePath))
                    # logFileModTime = time.strftime('%m/%d/%Y', time.gmtime(os.path.getmtime(self.logFilePath)) )
                    logFileModTime = datetime.datetime.fromtimestamp(
                        os.path.getmtime(self.logFilePath)
                    )
                    timeStamp = str(logFileModTime.year) + "/" + timeStamp

                    # Now parse time stamp
                    parsedTime = datetime.datetime.strptime(
                        timeStamp, "%Y/%m/%d %H:%M:%S "
                    )

                    return parsedTime

        # If here, didn't manage to parse
        return None

    def getReturnCodeFromFile(self, logFileText):

        # Parse return code from file
        returnCodeLength = 1
        key = "return value "
        keyPos = logFileText.find(key)
        if keyPos > -1:  # check found key
            returnCodePos = keyPos + len(key)
            if len(logFileText) > returnCodePos + 1:

                # Grad the return code, handling -ve values
                returnCode = logFileText[
                    returnCodePos : returnCodePos + returnCodeLength
                ]
                if returnCode == "-":
                    returnCode += logFileText[
                        returnCodePos
                        + returnCodeLength : returnCodePos
                        + returnCodeLength
                        + 1
                    ]
                return int(returnCode)

        # If here, didn't manage to parse
        return None

    def getMemoryUsageFromFile(self, logFileText):

        # Parse memory usage from file
        # Break into lines, find line with table containing memory info, then tokenise the oine and get the cells containing the memory used and requested
        key = "Memory (MB)"
        logFileLines = logFileText.split("\n")
        for line in logFileLines:
            if key in line:
                tokens = line.split()
                if len(tokens) == 6:
                    memoryUsed = int(tokens[3])
                    memoryRequested = int(tokens[4])
                    memoryAllocated = int(tokens[5])
                    return memoryUsed, memoryRequested, memoryAllocated

        # If here, didn't manage to parse
        return None, None, None


#
# Test
#

if __name__ == "__main__":

    from .job import ClusterCommand, ClusterJob

    test_dir = "./tmp/condor"
    make_dir(test_dir)

    exe_path = "echo"
    exe_args = [
        "bar",
        "bar",
    ]

    dagman_file, condor_file = create_dagman_submit_file(
        submit_dir=test_dir,
        log_dir=test_dir,
        dagman_file_name="test.dagman",
        condor_file_name="test.condor",
        exe_path=exe_path,
        exe_args=exe_args,
        memory=1000,
    )
